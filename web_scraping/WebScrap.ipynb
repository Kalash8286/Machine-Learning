{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_Scrapping.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOi6etYDxCXMoLVDqOKwNiA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kalash8286/Scraping/blob/main/web_scraping/WebScrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mON1xAP0wwO0"
      },
      "outputs": [],
      "source": [
        "# import files  \n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from bs4.element import NavigableString\n",
        "import csv  \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#variables\n",
        "subHeading, subHeadingList  =([] for i in range(2))\n",
        "intro = ''\n",
        "chapterName = ''\n",
        "\n",
        "#final answer list\n",
        "subHeadingIntro, subHeadings, allEaxmples =([] for i in range(3))"
      ],
      "metadata": {
        "id": "nuLXn2NEyo_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert list into string\n",
        "def listIntoString(lists):    \n",
        "    str1 =''\n",
        "    for ele in lists:         \n",
        "        str1 += ele  \n",
        "    return str1"
      ],
      "metadata": {
        "id": "qxu5txh7y-q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here all p tags are passed which includes text all other headings also\n",
        "# so here exatracting only needed text for \n",
        "def filterSubHeadingIntro(subHeadingIntro):\n",
        "    removeStr = len(subHeadingIntro)\n",
        "    i = -1\n",
        "    subIntro = []\n",
        "    lastItem = \"\"\n",
        "    while removeStr > 0:   \n",
        "        if( len(subHeadingIntro[i]) != 0 ):\n",
        "            if(len(subIntro) == 0):\n",
        "                subIntro.append(subHeadingIntro[i])\n",
        "                lastItem = subHeadingIntro[i]\n",
        "            else:\n",
        "                subIntro.append(subHeadingIntro[i].replace(lastItem, \"\"))\n",
        "                lastItem = subHeadingIntro[i]           \n",
        "        i -= 1\n",
        "        removeStr -= 1\n",
        "    subIntro.reverse()\n",
        "    return subIntro"
      ],
      "metadata": {
        "id": "FgtmO4SYy_NA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def findHeading(soup):    \n",
        "  # print(\"\\n==========PAGE HEADING==============\")\n",
        "  heading = soup.find_all('span', class_='color_h1')\n",
        "  if(heading != []):\n",
        "    heading = heading[0].get_text()\n",
        "  else:\n",
        "    heading = \"\"  \n",
        "  \n",
        "\n",
        "  # print(\"\\n==========INTRODUCTION==============\")\n",
        "  global intro \n",
        "  intro = soup.find_all('p', class_='intro')\n",
        "  \n",
        "  if(intro != []):\n",
        "    intro = intro[0].get_text()\n",
        "  else:\n",
        "    intro = \"\"  \n",
        "  \n",
        "  return heading"
      ],
      "metadata": {
        "id": "JVza274ty_nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def findSubHeading(soup):\n",
        "  global subHeadings\n",
        "  try:\n",
        "    # print(\"\\n==========ALL SUB HEADINGS==============\")\n",
        "    sub_heading = soup.find('hr')\n",
        "    sub_heading = sub_heading.find_next_siblings(\"h2\")\n",
        "    for h in sub_heading:\n",
        "        if (h.string.find('Exercises') != -1):\n",
        "            break    \n",
        "        elif(h.string == 'Chapter Summary'):\n",
        "            break\n",
        "        elif(h.string == 'HTML List Tags'):\n",
        "            break\n",
        "        else:\n",
        "            subHeadings.append(h.get_text())\n",
        "  except:\n",
        "    subHeadings.append('')"
      ],
      "metadata": {
        "id": "KzFXps_TzAiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def findIntroSubheading(soup):\n",
        "  global subHeadingIntro\n",
        "  # print(\"\\n==========INTRO FOR ALL SUB HEADINGS==============\")\n",
        "  try:\n",
        "    sub_heading_intro = soup.find('div', id='main')\n",
        "    length = len(sub_heading_intro.find_all('hr'))\n",
        "    i=0\n",
        "    while i < length:\n",
        "        sub = sub_heading_intro.find_all('hr')[i]\n",
        "        sub = sub.find_next_siblings(\"p\")\n",
        "        for k in sub:      \n",
        "            if(k.get_text() == intro):\n",
        "                continue\n",
        "            if(k.get_text().__contains__(\"W3Schools' tag reference contains\")):\n",
        "                continue\n",
        "            else:  \n",
        "                subHeadingList.append(k.get_text()) \n",
        "        str1 = listIntoString(subHeadingList)\n",
        "        subHeadingList.clear()  \n",
        "        if(str1 not in subHeadingIntro):\n",
        "            subHeadingIntro.append(str1)               \n",
        "        i += 1\n",
        "  except:\n",
        "    subHeadingIntro.append('')               \n",
        "    # sending data for filtering\n",
        "    subHeadingIntro = filterSubHeadingIntro(subHeadingIntro)"
      ],
      "metadata": {
        "id": "LPIVqlgxy_v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def findExample(soup):\n",
        "  global allEaxmples\n",
        "  dummyExamples = []\n",
        "  text = ''\n",
        "\n",
        "  # print(\"\\n==========ALL EXAMPLES FOR SUB HEADINGS==============\")\n",
        "  example = soup.select(\"div.w3-example div.notranslate\")\n",
        "  for e in example:    \n",
        "    example = False            \n",
        "    dummyExamples.append(e.get_text())        \n",
        "    par = e.parent     \n",
        "  \n",
        "    #under one heading can have multiple examples\n",
        "    for sibling in par.next_siblings: \n",
        "      # print(sibling)\n",
        "      for s in sibling:\n",
        "        if(str(s) == '<h3>Example</h3>'):\n",
        "          example = True\n",
        "      if(example == False):\n",
        "        if(str(sibling) == '<hr/>'):\n",
        "          dummyExamples.append(\"|||\") \n",
        "          break\n",
        "\n",
        "  # arranging all examples  \n",
        "  for e in dummyExamples:    \n",
        "    if(e == '|||'):\n",
        "      allEaxmples.append(text)\n",
        "      text = ''\n",
        "    else:\n",
        "      if(len(text) == 0):\n",
        "        text += e\n",
        "      else:\n",
        "        text += ' &&&&&& '+e "
      ],
      "metadata": {
        "id": "HJ6d9EKFy_4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def writeInCSV(chapterName, heading, url, subHeadings, subHeadingIntro, allEaxmples):   \n",
        "\n",
        "  \n",
        "  for k in range(len(subHeadings)):\n",
        "    if(k >= len(subHeadingIntro)):\n",
        "      subHeadingIntro.append(\"\")\n",
        "    if(k >= len(allEaxmples)):\n",
        "      allEaxmples.append(\"\")\n",
        " \n",
        "\n",
        "  with open('web.csv', 'a', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    counter = 0 \n",
        "    heading = chapterName +'||'+ heading.strip()     \n",
        "    # print(heading)\n",
        "    for head in subHeadings:\n",
        "      data = heading, url, subHeadings[counter], subHeadingIntro[counter], allEaxmples[counter]\n",
        "      writer.writerow(data)\n",
        "      counter += 1"
      ],
      "metadata": {
        "id": "nSOvGL4RBl7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header = ['Heading', 'link', 'Subheading', 'Definations', 'Example']\n",
        "with open('web.csv', 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)  \n",
        "\n"
      ],
      "metadata": {
        "id": "GJUlkSBvyhnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "webLinks = pd.read_csv('link.txt',sep='\\n',header=None)[0].tolist()\n",
        "\n",
        "num = 1\n",
        "\n",
        "for link in webLinks:  \n",
        "  chapterName = link.split(\"/\")[3]\n",
        "  r = requests.get(link)   \n",
        "  soup = BeautifulSoup(r.content, 'html.parser')\n",
        "  heading = findHeading(soup)\n",
        "  findSubHeading(soup)  \n",
        "  findIntroSubheading(soup)\n",
        "  findExample(soup)\n",
        "  writeInCSV(chapterName, heading, link, subHeadings, subHeadingIntro, allEaxmples)  \n",
        "  subHeadingIntro.clear()\n",
        "  subHeadings.clear()\n",
        "  allEaxmples.clear()  \n",
        "  print(num, 'RECORD DONE!')\n",
        "  num += 1\n"
      ],
      "metadata": {
        "id": "LH1q3gzwoUT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv(\"web.csv\")  \n",
        "# df.head(10)"
      ],
      "metadata": {
        "id": "LrTSBx-jE2wM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file1 = open(\"link.txt\",\"a\")\n",
        "\n",
        "# URL = \"https://www.w3schools.com/cpp/\"\n",
        "# r = requests.get(URL)   \n",
        "# soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "# divs  = soup.find(\"div\", id=\"leftmenuinnerinner\")\n",
        "# # soup.select(\"div.w3-example div.notranslate\")\n",
        "\n",
        "# for a in divs.find_all('a', href=True, target=\"_top\"):\n",
        "#   links = URL + a['href']\n",
        "#   file1.write(links)\n",
        "#   file1.write('\\n')\n",
        "#   print(links)\n",
        "\n",
        "# file1.close()"
      ],
      "metadata": {
        "id": "w9W9Fsp9w7Ld"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}